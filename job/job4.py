import random
import re
from datetime import datetime
import json
import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import pytesseract  # OCR è¯†åˆ«åº“
from io import BytesIO
from PIL import Image
from bs4 import BeautifulSoup
import cv2

pytesseract.pytesseract.tesseract_cmd = r"D:\2soft\ocr\tesseract.exe"
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.options import Options
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import numpy as np

# é…ç½®åŸºæœ¬ä¿¡æ¯
BASE_URL = "https://www.zjzrzyjy.com"
UNIT_DETAILS_URL = BASE_URL + "/trade/view/landbidding/queryResourceDetail?resourceId="
PUBLICITY_URL = f"{BASE_URL}/trade/view/landbidding/querylandbidding?currentPage=5&pageSize=100&regionCode=330200%2C330201%2C330203%2C330205%2C330211%2C330206%2C330212%2C330283%2C330281%2C330282%2C330226%2C330225&sortWay=desc&sortField=ZYKSSJ"
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36"
]

# OCR è¯†åˆ«ç›®æ ‡å…³é”®è¯
TARGET_KEYWORDS = ["ä½å®…", "åŠå…¬", "å‚æˆ¿", "å•†ä¸š", "å·¥ä¸š", "è½¦ä½", "å…¶ä»–"]
# HEADERS = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
#     "Referer": "https://newhouse.cnnbfdc.com/",
#     "Accept-Language": "zh-CN,zh;q=0.9",
#     "Accept-Encoding": "gzip, deflate, br",
# }
# COOKIES = {
#     "Hm_lvt_b33309ddse6aedc0b7a6a5e11379efd": "55048fdc8ccdde7cc81b097494237b631d7b88b3b7b320d38bca4987f33e8725c589288536ac528130c517ed9ec2a3d8fc7a5b5f8cfc294d732e075530ba3fcdc8fa0ce8cab62110ee9e43cd5307a0bd75337066cfb34b2955ed9a7d347add81935e1459f7e12b95094cbde1fa5e6165fb7d55ca993e3aab1a4efb298883afa3a106af589b1d15d46a7b629bd6b52f8970d91a4add43a0328db23ea1360c076d34deebac56bbae509fd0167363066d6f",
# }
COOKIES = {
    # "__51vcke__3Fu75AYmiUrDkM8l": "20b95220-c9e6-5195-b56f-f7489efe38ac",
    # "__51vuft__3Fu75AYmiUrDkM8l": "1711002659610",
    # "__51uvsct__3Fu75AYmiUrDkM8l": "2",
    "_wzd_nevertip_": "1/",
    "acw_tc": "0aef82d717516091611174131e005b16a5fa235a27a9b7ac87a0351e46295a",
}
COOKIES1 = [
    {"name": "Hm_lvt_b33309ddse6aedc0b7a6a5e11379efd",
     "value": "55048fdc8ccdde7cc81b097494237b631d7b88b3b7b320d38bca4987f33e87252b56735576e8e86ed22f7cbf917998b5789059c06c845c49af3383e2900d39cc083d76312697924265af1c2732b1e66a92cfd48b5fdc002f6c760ce5817bc2a1ca2b68241748bbe1b83ed8fedcf3cbe4ef7e893b5a70e84359f1e28ab459cee16a550ba54f29c4819066197e46e81b87a5b360d831add14631f7ff2b4237ea431d9a993068720b04a96f2289d334a6f1"}
]

HEADERS = {"User-Agent": random.choice(USER_AGENTS),
           # "X-Requested-With": "XMLHttpRequest",
           "Accept": "application/json",  # æŒ‡å®šè¿”å› JSON
           "Referer": "https://www.zjzrzyjy.com",
           }
EDGE_DRIVER_PATH = "D:/2soft/edgedriver/msedgedriver.exe"
# é…ç½® Selenium WebDriver

# é…ç½® Chrome
chrome_options = Options()
chrome_options.add_argument("--headless")  # æ— å¤´æ¨¡å¼
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-blink-features=AutomationControlled")  # å Selenium æ£€æµ‹
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

# service = Service("D:/2soft/chromedriver/chromedriver.exe")  # ä½ çš„ ChromeDriver è·¯å¾„
# driver = webdriver.Chrome(service=service, options=chrome_options)

options = Options()
options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
options.add_argument("--disable-gpu")

# æ‰‹åŠ¨æŒ‡å®š WebDriver ç«¯å£ï¼Œé¿å…ç«¯å£éšæœºå¯¼è‡´è¿æ¥å¤±è´¥
# service = EdgeService(EDGE_DRIVER_PATH)
# service.start()  # æ˜¾å¼å¯åŠ¨æœåŠ¡
# driver = webdriver.Edge(service=service, options=options)
# âœ… è®¾å®šå…¨å±€å˜é‡ï¼Œå­˜å‚¨ WebDriver
_driver = None


# def get_driver():
#     """è·å– WebDriver å®ä¾‹ï¼Œç¡®ä¿ WebDriver åªåˆå§‹åŒ–ä¸€æ¬¡"""
#     global _driver
#     if _driver is None:
#         options = Options()
#         options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
#         service = EdgeService("D:/2soft/edgedriver/msedgedriver.exe")  # EdgeDriver è·¯å¾„
#         _driver = webdriver.Edge(service=service, options=options)
#     return _driver
def get_driver():
    """æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„ WebDriverï¼Œç¡®ä¿ä¸ä¼šè¶…æ—¶"""
    # options = Options()
    # options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    options = Options()
    options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")  # å Selenium æ£€æµ‹
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
    )
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
    )
    service = EdgeService("D:/2soft/edgedriver/msedgedriver.exe")  # EdgeDriver è·¯å¾„
    driver = webdriver.Edge(service=service, options=options)
    return driver  # âš ï¸ å…³é”®å˜åŒ–ï¼šæ¯æ¬¡éƒ½è¿”å›ä¸€ä¸ªæ–°çš„ driver


def load_page_with_cookies(driver, url, cookies):
    """
    ä½¿ç”¨ Selenium åŠ è½½é¡µé¢ï¼Œå¹¶æ·»åŠ  Cookies ä»¥ä¿æŒä¼šè¯ã€‚

    :param driver: WebDriver å®ä¾‹
    :param url: ç›®æ ‡ç½‘é¡µ URL
    :param cookies: Cookies å­—å…¸
    """
    driver.get(url)  # æ‰“å¼€ç½‘é¡µ

    # âœ… é¿å…é‡å¤æ³¨å…¥ Cookies
    if not hasattr(driver, "_cookies_loaded"):
        cookies_list = [{"name": key, "value": value} for key, value in cookies.items()]
        for cookie in cookies_list:
            driver.add_cookie(cookie)
        driver.refresh()  # åˆ·æ–°é¡µé¢ï¼Œä½¿ Cookies ç”Ÿæ•ˆ
        driver._cookies_loaded = True  # æ ‡è®° Cookies å·²åŠ è½½

    # time.sleep(3)  # é€‚å½“ç­‰å¾…é¡µé¢åŠ è½½


# è·å–è®¸å¯è¯åˆ—è¡¨
def get_permits(limit=500):
    response = requests.get(PUBLICITY_URL, headers=HEADERS, cookies=COOKIES)
    # time.sleep(1)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    permits = []
    table = soup.find("table", class_="layui-table")
    if not table:
        print("æœªæ‰¾åˆ°è®¸å¯ä¿¡æ¯è¡¨æ ¼")
        return []

    rows = table.find("tbody").find_all("tr")
    for row in rows[:limit]:  # é™åˆ¶è·å–çš„è®¸å¯è¯æ•°é‡
        columns = row.find_all("td")
        permit_name = columns[0].text.strip()
        permit_date = columns[1].text.strip()
        project_name = columns[2].text.strip()
        district = columns[3].text.strip()
        developer = columns[4].text.strip()
        # è·å–è¯¦æƒ…å’Œå®æ—¶æ•°æ®é¡µé¢é“¾æ¥
        detail_href = columns[0].find("a")["href"]
        detail_url = BASE_URL + detail_href
        realdata_url = detail_url.replace("Details", "realdata")
        # âœ… æå–å¹´ä»½å’Œæœˆä»½
        try:
            date_obj = datetime.strptime(permit_date, "%Y-%m-%d")
            year = date_obj.year
            month = date_obj.month
        except ValueError:
            year, month = "æœªçŸ¥", "æœªçŸ¥"
        permits.append({
            "è®¸å¯è¯åç§°": permit_name,
            "è®¸å¯æ—¥æœŸ": permit_date,
            "å¹´ä»½": year,  # âœ… æ–°å¢åˆ—
            "æœˆä»½": month,  # âœ… æ–°å¢åˆ—
            "é¡¹ç›®åç§°": project_name,
            "åŒºåŸŸ": district,
            "å¼€å‘ä¼ä¸š": developer,
            "è¯¦æƒ…é¡µ": detail_url,
            "å®æ—¶æ•°æ®é¡µ": realdata_url,
        })
    print(permits)
    return permits


def get_text_from_image(element):
    """æˆªå›¾å¹¶ä½¿ç”¨ OCR è§£æå¤‡æ¡ˆå¥—æ•°"""
    element.screenshot("å¤‡æ¡ˆå¥—æ•°.png")
    image = Image.open("å¤‡æ¡ˆå¥—æ•°.png")
    return pytesseract.image_to_string(image, config="--psm 6").strip()


def extract_number(text):
    """ä»å­—ç¬¦ä¸²ä¸­æå–æ•°å­—å¹¶è¿”å› float ç±»å‹"""
    match = re.search(r'\d+(\.\d+)?', text)  # åŒ¹é…æ•´æ•°æˆ–å°æ•°
    return float(match.group()) if match else 0  # ç¡®ä¿è¿”å› float


# def get_text_from_image(img_url):
#     """ä¸‹è½½å›¾ç‰‡å¹¶ä½¿ç”¨ OCR æå–æ–‡å­—"""
#     try:
#         response = requests.get(img_url, headers=HEADERS, stream=True)
#         if response.status_code == 200:
#             image = Image.open(BytesIO(response.content))
#             text = pytesseract.image_to_string(image, config="--psm 6 digits")  # åªè¯†åˆ«æ•°å­—
#             return extract_number(text)
#         else:
#             return "å›¾ç‰‡ä¸‹è½½å¤±è´¥"
#     except Exception as e:
#         return f"OCR å¤±è´¥: {str(e)}"
# è§£æè¯¦æƒ…é¡µä¿¡æ¯
def parse_project_details(url):
    response = requests.get(url, headers=HEADERS, cookies=COOKIES)
    time.sleep(3)
    # if response.status_code != 200:
    #     return {"é¡¹ç›®åç§°": "è·å–å¤±è´¥"}
    print(response.status_code)
    soup = BeautifulSoup(response.text, "html.parser")
    details = {}

    # è·å–é¡¹ç›®åç§°
    # title_element = soup.find("big", class_="bold")
    # details["é¡¹ç›®åç§°"] = title_element.text.strip() if title_element else "æœªçŸ¥"

    # è·å–ä½å®…å¤‡æ¡ˆå‡ä»·
    price_element = soup.find("big", class_="color--red bold fs24")
    details["ä½å®…å¤‡æ¡ˆå‡ä»·"] = price_element.text.strip() if price_element else "æœªçŸ¥"

    # è·å–é¡¹ç›®åœ°å€
    address_element = soup.find("span", title=True)
    details["é¡¹ç›®åœ°å€"] = address_element.text.strip() if address_element else "æœªçŸ¥"
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… "è¡—é“"ï¼ˆå¿½ç•¥ç©ºæ ¼ã€HTMLå®ä½“ï¼‰
    street_element = soup.find("em", string=re.compile(r"è¡—\s*é“"))

    # è·å–è¡—é“ä¿¡æ¯
    details["æ‰€åœ¨åŒº"] = street_element.find_next_sibling("span").text.strip() if street_element else "æœªçŸ¥"
    # è·å–å¼€å‘å•†
    # developer_element = soup.find("span").find("a")
    # details["å¼€å‘ä¼ä¸š"] = developer_element.text.strip() if developer_element else "æœªçŸ¥"
    # æå–å»ºç­‘ç±»å‹
    # è·å–å»ºç­‘ç±»å‹
    building_type = soup.find("em", string="å»ºç­‘ç±»å‹ï¼š")
    details["å»ºç­‘ç±»å‹"] = building_type.find_next_sibling("span").text.strip() if building_type else "æœªçŸ¥"

    # è·å–è£…ä¿®æ ‡å‡†
    decoration_standard = soup.find("em", string="è£…ä¿®æ ‡å‡†ï¼š")
    details["è£…ä¿®æ ‡å‡†"] = decoration_standard.find_next_sibling("span").text.strip() if decoration_standard else "æœªçŸ¥"
    # è·å–å»ºç­‘é¢ç§¯
    area_element = soup.find("li", string=lambda text: text and "å»ºç­‘é¢ç§¯ï¼š" in text)
    details["é¢„å”®å»ºç­‘é¢ç§¯"] = area_element.text.split("ï¼š")[-1].strip() if area_element else "æœªçŸ¥"
    # âœ… è·å–é¡¹ç›®ç®€ä»‹
    profile_input = soup.find("input", id="proj-text")
    details["é¡¹ç›®ç®€ä»‹"] = profile_input["value"].strip() if profile_input else "æœªçŸ¥"
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å« "è®¸å¯è¯å·" çš„ <a> æ ‡ç­¾
    # first_permit = soup.find("a", href=True, string=lambda text: text and permit["è®¸å¯è¯åç§°"] in text)
    # details["è®¸å¯è¯href"] = first_permit["href"] if first_permit else "æœªçŸ¥"
    try:
        # âœ… è·å– WebDriverï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        driver = get_driver()
        load_page_with_cookies(driver, url, COOKIES)
        # å…ˆç”¨ requests è·å– Cookie
        # session = requests.Session()
        # session.get(url, headers={"User-Agent": "Mozilla/5.0"})
        # cookies = session.cookies.get_dict()
        # print(cookies)
        # ä¼ é€’ Cookie ç»™ Selenium
        # **éå† COOKIES_LIST é€ä¸ªæ·»åŠ **
        # driver.get(url)
        # COOKIES_LIST = [{"name": key, "value": value} for key, value in COOKIES.items()]
        # # print(COOKIES_LIST)
        # for cookie in COOKIES_LIST:
        #     driver.add_cookie(cookie)  # âœ… ç¡®ä¿æ ¼å¼æ­£ç¡®
        #
        # time.sleep(3)  # é€‚å½“ç­‰å¾…é¡µé¢åŠ è½½
        # è·å–å¤‡æ¡ˆå¥—æ•°
        try:
            # img_element = driver.find_element(By.CLASS_NAME, "totalresidentialhousingcount")
            # details["é¢„å”®å¥—æ•°"] = get_text_from_image(img_element)
            # âœ… ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

            # âœ… ç­‰å¾…ç›®æ ‡å›¾ç‰‡å…ƒç´ å¯è§
            img_element = WebDriverWait(driver, 10).until(
                EC.visibility_of_element_located((By.CLASS_NAME, "totalresidentialhousingcount"))
            )

            # âœ… é‡æ–°æ£€æŸ¥å°ºå¯¸
            # width = img_element.size["width"]
            # height = img_element.size["height"]
            # if width == 0 or height == 0:
            #     return "OCR è§£æå¤±è´¥: å›¾ç‰‡å°ºå¯¸ä¸º 0"

            # âœ… æˆªå›¾å¹¶ OCR è§£æ
            details["é¢„å”®å¥—æ•°"] = get_text_from_image(img_element)
        except Exception as e:
            print(f"OCR è§£æå¤±è´¥: {e}")
            details["é¢„å”®å¥—æ•°"] = "OCR è§£æå¤±è´¥"

        return details
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return details


# è§£æå®æ—¶æ•°æ®é¡µä¿¡æ¯
@retry(
    stop=stop_after_attempt(5),  # æœ€å¤§é‡è¯• 5 æ¬¡
    wait=wait_exponential(multiplier=1, min=2, max=30),  # æŒ‡æ•°é€€é¿ï¼ˆ2s, 4s, 8s...æœ€å¤š 30sï¼‰
    retry=retry_if_exception_type(Exception)  # åªå¯¹ Exception è¿›è¡Œé‡è¯•
)
def parse_realdata(url):
    # print(1)
    response = requests.get(url, headers=HEADERS, cookies=COOKIES)
    time.sleep(4)
    if response.status_code == 503:
        print("503 æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè§¦å‘é‡è¯•...")
        raise Exception("503 Service Unavailable")  # è§¦å‘é‡è¯•
    # if response.status_code != 200:
    #     return {"è®¸å¯é¢ç§¯": "è·å–å¤±è´¥", "å·²å”®é¢ç§¯": "è·å–å¤±è´¥", "å½“å‰å¯å”®å¥—æ•°": "è·å–å¤±è´¥"}
    print(response.status_code)
    soup = BeautifulSoup(response.text, "html.parser")
    realdata = {}

    # è·å–è®¸å¯ã€å·²å”®ã€å¯å”®æ•°æ®
    data_summary = soup.find("ul", class_="data-summary-list")
    if data_summary:
        items = data_summary.find_all("li")
        # realdata["é¢„å”®å»ºç­‘é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰"] = extract_number(items[1].text.strip() if len(items) > 1 else "æœªçŸ¥")
        # realdata["é¢„å”®å¥—æ•°"] = extract_number(items[-2].text.strip() if len(items) > 2 else "æœªçŸ¥")
        realdata["è®¸å¯è¯é¢ç§¯"] = extract_number(items[0].text.strip() if len(items) > 0 else "æœªçŸ¥")
        realdata["è®¸å¯å¥—æ•°"] = extract_number(items[6].text.strip() if len(items) > 6 else "æœªçŸ¥")
    return realdata


@retry(
    stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯• 3 æ¬¡
    wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿ï¼ˆ2s, 4s, 8sï¼‰
    retry=retry_if_exception_type(requests.exceptions.RequestException),  # åªå¯¹è¯·æ±‚å¼‚å¸¸é‡è¯•
    retry_error_callback=lambda _: {"ä¸šæ€": "æœªçŸ¥"}  # å½“ 3 æ¬¡éƒ½å¤±è´¥æ—¶è¿”å›é»˜è®¤å€¼
)
def parse_property_type(url):
    """
    è§£ææ¥¼ç›˜è¯¦æƒ…é¡µï¼Œè·å–ä»»æ„æˆ¿å·çš„ `data-guid`ï¼Œè®¿é—® `unit/details` é¡µé¢ï¼Œ
    OCR è¯†åˆ«æˆ¿å±‹ä¸šæ€ï¼ˆå¦‚ ä½å®…ã€åŠå…¬ç­‰ï¼‰ã€‚
    :param url: æ¥¼ç›˜ä¿¡æ¯é¡µé¢ URL
    :return: ä¸šæ€ä¿¡æ¯ï¼Œå¦‚ `{"ä¸šæ€": "ä½å®…"}`
    """
    details = {"ä¸šæ€": "å…¶ä»–"}  # é»˜è®¤å€¼
    headers = HEADERS.copy()  # é¿å…å…¨å±€ä¿®æ”¹
    headers["Referer"] = url
    # 1. è®¿é—®æ¥¼ç›˜ä¿¡æ¯é¡µ
    response = requests.get(url, headers=HEADERS, cookies=COOKIES)
    time.sleep(4)
    if response.status_code == 503:
        print("503 æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè§¦å‘é‡è¯•...")
        raise Exception("503 Service Unavailable")  # è§¦å‘é‡è¯•
    result = {"ä¸šæ€": "æœªçŸ¥"}  # è®¾å®šé»˜è®¤å€¼ï¼Œç»§ç»­æ‰§è¡Œç¨‹åº
    # 2. æå– `data-guid`
    soup = BeautifulSoup(response.text, "html.parser")
    # print(soup.prettify())
    house_links = soup.select(".house-title a[data-guid]")  # æ‰€æœ‰æˆ¿å±‹é“¾æ¥
    if not house_links:
        print("âš ï¸ æœªæ‰¾åˆ°æˆ¿å· `data-guid`ï¼Œè·³è¿‡è§£æ")
        return details
    unit_guid = house_links[0].get("data-guid")

    # 3. è®¿é—® `unit/details`
    unit_url = UNIT_DETAILS_URL + unit_guid
    print(f"ğŸ” è·å–å•å…ƒè¯¦æƒ…é¡µ: {unit_url}")

    try:
        unit_response = requests.get(unit_url, headers=headers, cookies=COOKIES)
        unit_response.raise_for_status()
        unit_soup = BeautifulSoup(unit_response.text, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"âŒ å•å…ƒè¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {e}")
        return details

    # 4. æå–å›¾ç‰‡ URL
    img_element = unit_soup.find("img")
    if not img_element or "src" not in img_element.attrs:
        print("âš ï¸ æœªæ‰¾åˆ°æˆ¿å±‹å›¾ç‰‡ï¼Œè·³è¿‡ OCR è¯†åˆ«")
        return details
    img_url = img_element["src"]

    # å¤„ç†ç›¸å¯¹è·¯å¾„
    if img_url.startswith("/"):
        img_url = BASE_URL + img_url

    # 5. OCR è¯†åˆ«
    try:
        img_response = requests.get(img_url, headers=HEADERS, stream=True, cookies=COOKIES)
        if img_response.status_code == 200:
            # è¯»å–å›¾ç‰‡æ•°æ®å¹¶è½¬æ¢ä¸º OpenCV æ ¼å¼
            image = Image.open(BytesIO(img_response.content))
            image_np = np.array(image)  # PIL â†’ NumPy
            # gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)  # è½¬æ¢ä¸ºç°åº¦å›¾

            # é¢„å¤„ç†ï¼šå»å™ª & å¢å¼ºå¯¹æ¯”åº¦
            # blurred = cv2.GaussianBlur(gray, (3, 3), 0)
            # binary = cv2.adaptiveThreshold(
            #     blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
            # )

            # OCR è¯†åˆ«
            text = pytesseract.image_to_string(image, lang="chi_sim", config="--psm 6").strip()
            # print(image)
            # print(text)
        else:
            print("âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥")
            return details
    except Exception as e:
        print(f"âš ï¸ OCR å¤±è´¥: {e}")
        return details

    # 6. ç¡®å®šä¸šæ€
    for keyword in TARGET_KEYWORDS:
        if keyword in text:
            details["ä¸šæ€"] = keyword
            break  # æ‰¾åˆ°å³åœæ­¢
    return details


response = requests.get(url=PUBLICITY_URL, headers=HEADERS, cookies=COOKIES)

response_bytes = response.content
# 1ï¸âƒ£ è§£ç æˆå­—ç¬¦ä¸²ï¼ˆUTF-8ï¼‰
response_str = response_bytes.decode("utf-8")

# 2ï¸âƒ£ è§£æ JSON
response_json = json.loads(response_str)
# **1ï¸âƒ£ å®šä¹‰ Excel åˆ—åå’Œ JSON å¯¹åº”å­—æ®µ**
column_mapping = {
    "å‡ºè®©æ–¹å¼": "bidWay",  # äº¤æ˜“æ–¹å¼
    "åŒºåŸŸ": "xzqName",  # è¡Œæ”¿åŒºåç§°
    "ç”¨é€”": "planUse",  # åœŸåœ°ç”¨é€”
    "å…¬å‘Šæ—¶é—´": "ggPubTime",  # å…¬å‘Šå‘å¸ƒæ—¶é—´
    "æˆäº¤æ—¶é—´":"clinchConfirmTime",  # "2024-12-31 16:00:00"
    "åœ°å—ç¼–å·": "resourceNumber",  # èµ„æºç¼–å·
    "åœ°å—åç§°": "resourceName",  # èµ„æºåç§°
    "åœŸåœ°ä½ç½®": "resourceLocation",  # èµ„æºä½ç½®
    "åœŸåœ°ç”¨é€”": "planUse",  # ç”¨é€”ï¼ˆå¯èƒ½é‡å¤ï¼Œå¯åˆ é™¤ï¼‰
    "å‡ºè®©é¢ç§¯": "landArea",  # åœŸåœ°é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰
    "ç«ä¹°ä¿è¯é‡‘ï¼ˆä¸‡å…ƒï¼‰": "bond",  # ä¿è¯é‡‘ï¼ˆä¸‡å…ƒï¼‰
    "å®¹ç§¯ç‡": "rjl",  # å®¹ç§¯ç‡
    "å®¹ç§¯ç‡åŒºé—´": "rjl1",  # å®¹ç§¯ç‡
    "ç«åœ°ä»·èµ·å§‹ä»·": "rjl2",  # å®¹ç§¯ç‡
    "ç«å¾—å•ä½": "theUnit",  # å®¹ç§¯ç‡
    "èµ·å§‹ä»·": "startPrice",  # èµ·å§‹ä»·æ ¼ï¼ˆä¸‡å…ƒï¼‰
    "æˆäº¤ä»·": "dealPrice",  # å®¹ç§¯ç‡
    "ç»¿åŒ–ç‡": "greenRate",  # å®¹ç§¯ç‡
    "å»ºç­‘å¯†åº¦": "buildDensity",  # å®¹ç§¯ç‡
    "å»ºé«˜": "heightLimit",  # å®¹ç§¯ç‡
    "å›ºå®šèµ„äº§æŠ•èµ„å¼ºåº¦": "investIntensityValue",  # å®¹ç§¯ç‡
    "äº©å‡ç¨æ”¶": "perAcreTaxValue",  # å®¹ç§¯ç‡
}
record_list = []
def parse_build_density(build_density_json: str) -> str:
    """è§£æ buildDensity å­—æ®µï¼Œç”Ÿæˆå»ºç­‘å¯†åº¦æè¿°ï¼ˆå°å€¼åœ¨å‰ï¼‰"""
    try:
        data = json.loads(build_density_json)
    except json.JSONDecodeError as e:
        raise ValueError(f"buildDensityå­—æ®µæ ¼å¼é”™è¯¯: {e}")

    lower_symbol_raw = data.get("JZMD_X_FH")
    lower_value_raw = data.get("JZMD_X")
    upper_symbol_raw = data.get("JZMD_S_FH")
    upper_value_raw = data.get("JZMD_S")

    parts = []

    # å¤„ç†ä¸‹é™
    if lower_symbol_raw and lower_value_raw:
        try:
            lower_value = float(lower_value_raw)
            parts.append(f"{lower_value:.1f}%{lower_symbol_raw}å»ºç­‘å¯†åº¦")
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„å»ºç­‘å¯†åº¦ä¸‹é™æ•°å€¼: {lower_value_raw}")

    # å¤„ç†ä¸Šé™
    if upper_symbol_raw and upper_value_raw:
        try:
            upper_value = float(upper_value_raw)
            if parts:
                parts.append(f"{upper_symbol_raw}{upper_value:.1f}%")
            else:
                parts.append(f"å»ºç­‘å¯†åº¦{upper_symbol_raw}{upper_value:.1f}%")
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„å»ºç­‘å¯†åº¦ä¸Šé™æ•°å€¼: {upper_value_raw}")

    if parts:
        return "".join(parts)
    else:
        return "æ— å»ºç­‘å¯†åº¦é™åˆ¶"

def parse_height_limit(height_limit_json: str) -> str:
    """è§£æ heightLimit å­—æ®µï¼Œç”Ÿæˆå»ºç­‘é™é«˜æè¿°ï¼ˆå°å€¼åœ¨å‰ï¼‰"""

    try:
        data = json.loads(height_limit_json)
    except json.JSONDecodeError as e:
        raise ValueError(f"heightLimitå­—æ®µæ ¼å¼é”™è¯¯: {e}")

    lower_symbol_raw = data.get("XG_S_FH")
    lower_value_raw = data.get("XG_S")
    upper_symbol_raw = data.get("XG_X_FH")
    upper_value_raw = data.get("XG_X")

    parts = []

    # å…ˆå¤„ç†ä¸‹é™
    if lower_symbol_raw and lower_value_raw:
        try:
            lower_value = float(lower_value_raw)
            parts.append(f"{lower_value:.1f}ç±³{lower_symbol_raw}å»ºç­‘é™é«˜")
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„ä¸‹é™æ•°å€¼: {lower_value_raw}")

    # å†å¤„ç†ä¸Šé™
    if upper_symbol_raw and upper_value_raw:
        try:
            upper_value = float(upper_value_raw)
            if parts:
                parts.append(f"{upper_symbol_raw}{upper_value:.1f}ç±³")
            else:
                parts.append(f"å»ºç­‘é™é«˜{upper_symbol_raw}{upper_value:.1f}ç±³")
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„ä¸Šé™æ•°å€¼: {upper_value_raw}")

    if parts:
        return "".join(parts)
    else:
        return "æ— å»ºç­‘é™é«˜é™åˆ¶"

def parse_green_rate_strict(green_rate_json: str):
    """ä¸¥æ ¼æŒ‰ç…§å°å€¼åœ¨å‰ï¼Œç»¿åŒ–ç‡è§£ææ ¼å¼"""

    try:
        data = json.loads(green_rate_json)
    except json.JSONDecodeError as e:
        raise ValueError(f"greenRateå­—æ®µæ ¼å¼é”™è¯¯: {e}")

    lower_symbol_raw = data.get("LHL_X_FH")
    lower_value_raw = data.get("LHL_X")
    upper_symbol_raw = data.get("LHL_S_FH")
    upper_value_raw = data.get("LHL_S")

    parts = []

    # å…ˆå¤„ç†ä¸‹é™
    if lower_symbol_raw and lower_value_raw:
        try:
            lower_value = float(lower_value_raw)
            parts.append(f"{lower_value:.1f}%{lower_symbol_raw}ç»¿åŒ–ç‡")
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„ä¸‹é™æ•°å€¼: {lower_value_raw}")

    # å†å¤„ç†ä¸Šé™
    if upper_symbol_raw and upper_value_raw:
        try:
            upper_value = float(upper_value_raw)
            if parts:
                parts.append(f"{upper_symbol_raw}{upper_value:.1f}%")
            else:
                parts.append(f"ç»¿åŒ–ç‡{upper_symbol_raw}{upper_value:.1f}%")
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„ä¸Šé™æ•°å€¼: {upper_value_raw}")

    if parts:
        return "".join(parts)
    else:
        return "æ— ç»¿åŒ–ç‡é™åˆ¶"
def get_auction_type(value: str) -> str:
    return "æ‹å–" if value == "ZJ" else "æŒ‚ç‰Œ" if value == "YPFM" else "æœªçŸ¥"


def classify_land_type(field: str) -> str:
    field = field.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œé˜²æ­¢è¯¯åˆ¤

    # è¯†åˆ«å•†ä¸šç›¸å…³çš„å…³é”®è¯
    commercial_keywords = {"å•†ä¸š", "å•†åŠ¡", "å¨±ä¹", "æ‰¹å‘", "é›¶å”®", "é¤é¥®", "æ—…é¦†", "è¥ä¸š"}
    has_commercial = any(keyword in field for keyword in commercial_keywords) or ("40å¹´" in field)
    has_residential = "ä½" in field or ("70å¹´" in field)
    has_industrial = "å·¥" in field or ("åº“" in field) or ("50å¹´" in field)

    if has_commercial and has_residential:
        return "å•†ä½"
    elif has_commercial:
        return "å•†æœ"
    elif has_residential:
        return "ä½å®…"
    elif has_industrial:
        return "å·¥ä¸š"
    else:
        return "å…¶ä»–"


# 1ï¸âƒ£ éå† recordsï¼Œæå– resourceId
for record in response_json["data"]["records"]:
    # print(record["resourceId"])
    url1 = UNIT_DETAILS_URL + record["resourceId"]
    response = requests.get(url=url1, headers=HEADERS, cookies=COOKIES)
    response_bytes = response.content
    # 1ï¸âƒ£ è§£ç æˆå­—ç¬¦ä¸²ï¼ˆUTF-8ï¼‰
    response_str = response_bytes.decode("utf-8")

    # 2ï¸âƒ£ è§£æ JSON
    response_json = json.loads(response_str)
    # print(response_json["data"]["bidWay"])
    row_data = {}
    for col_name, json_key in column_mapping.items():
        # value = record.get(json_key, "æœªçŸ¥")
        if col_name in ["å‡ºè®©æ–¹å¼"]:
            value = response_json["data"]["bidWay"]
            value = get_auction_type(value)
            row_data[col_name] = value
        if col_name in ["åŒºåŸŸ"]:
            value = response_json["data"]["administrativeRegioncode"]
            row_data[col_name] = value
        if col_name in ["ç”¨é€”"]:
            value = response_json["data"]["assignmentPeriod"]
            value1 = classify_land_type(value)
            row_data[col_name] = value1
        if col_name in ["å…¬å‘Šæ—¶é—´"]:
            value = response_json["data"]["entryTime"]
            row_data[col_name] = value
        if col_name in ["æˆäº¤æ—¶é—´"]:
            value = response_json["data"]["clinchConfirmTime"]
            row_data[col_name] = value
        if col_name in ["åœ°å—ç¼–å·"]:
            value = response_json["data"]["resourceNumber"]
            row_data[col_name] = value
        if col_name in ["åœ°å—åç§°"]:
            value = response_json["data"]["resourceName"]
            row_data[col_name] = value
        if col_name in ["åœŸåœ°ä½ç½®"]:
            value = response_json["data"]["resourceLocation"]
            row_data[col_name] = value
        if col_name in ["åœŸåœ°ç”¨é€”"]:
            value = response_json["data"]["assignmentPeriod"]
            row_data[col_name] = value
        if col_name in ["å‡ºè®©é¢ç§¯"]:
            value = response_json["data"]["assignmentArea"]
            row_data[col_name] = value
        # if col_name in ["èµ·å§‹æ€»ä»·"]:
        #     b = response_json["data"]["bidRuleVO"]
        #
        #     if response_json["data"]["bidRuleVO"].get("stageOneOriginPrice"):
        #         b1 = b.get("stageOneOriginPrice")
        #         b2 = b.get("stageOneUint")
        #         if b2 in ["ä¸‡å…ƒ"]:
        #             row_data[col_name] = b1
        #         else:
        #             s = response_json["data"]["plotRatio"]
        #             print(b1)
        #             s1 = json.loads(s).get("RJL_S")
        #             extract_number(b1) * float(extract_number(s1))/10000
        #     else:
        #         row_data[col_name] = "æœªçŸ¥"
        # if col_name in ["ç«ä¹°ä¿è¯é‡‘ï¼ˆä¸‡å…ƒï¼‰"]:
        #     value = response_json["data"]["bail"]
        #     row_data[col_name] = value
        if col_name in ["å®¹ç§¯ç‡"]:
            s = response_json["data"]["plotRatio"]
            # print(s)
            value = json.loads(s).get("RJL_S")
            row_data[col_name] = value
        if col_name in ["å®¹ç§¯ç‡åŒºé—´"]:
            s = response_json["data"]["plotRatio"]
            s1 = json.loads(s).get("RJL_X")
            s2 = json.loads(s).get("RJL_S")
            # print(s)

            row_data[col_name] = s1+"-"+s2
        if col_name in ["ç«åœ°ä»·èµ·å§‹ä»·"]:
            b=response_json["data"]["bidRuleVO"]

            if response_json["data"]["bidRuleVO"].get("stageOneOriginPrice"):
                b1 = b.get("stageOneOriginPrice")
                b2=b.get("stageOneUint")
                if b2 in ["ä¸‡å…ƒ"]:
                    row_data[col_name] = b1*10000
                else :
                    row_data[col_name] = b1
            else:
                row_data[col_name] = "æœªçŸ¥"
        if col_name in ["ç«å¾—å•ä½"]:
            value = response_json["data"]["theUnit"]
            row_data[col_name] = value
        if col_name in ["èµ·å§‹ä»·"]:
            value = response_json["data"]["startPrice"]
            row_data[col_name] = value
        if col_name in ["æˆäº¤ä»·"]:
            value = response_json["data"]["dealPrice"]
            row_data[col_name] = value
        if col_name in ["ç»¿åŒ–ç‡"]:
            value = response_json["data"]["greenRate"]
            row_data[col_name] = parse_green_rate_strict(value)
        if col_name in ["å»ºç­‘å¯†åº¦"]:
            value = response_json["data"]["buildDensity"]
            row_data[col_name] = parse_build_density(value)
        if col_name in ["å»ºé«˜"]:
            value = response_json["data"]["heightLimit"]
            row_data[col_name] = parse_height_limit(value)
        if col_name in ["å›ºå®šèµ„äº§æŠ•èµ„å¼ºåº¦"]:
            value = response_json["data"]["investIntensityValue"]
            row_data[col_name] = value
        if col_name in ["äº©å‡ç¨æ”¶"]:
            value = response_json["data"]["perAcreTaxValue"]
            row_data[col_name] = value
    record_list.append(row_data)

# **4ï¸âƒ£ åˆ›å»º DataFrame å¹¶è®¾ç½®åˆ—å**
df = pd.DataFrame(record_list)

# **5ï¸âƒ£ ä¿å­˜åˆ° Excel**
df.to_excel("record_list.xlsx", index=False)

print("âœ… Excel æ–‡ä»¶å·²ä¿å­˜ï¼šrecord_list.xlsx")
# options = webdriver.EdgeOptions()
# options.add_argument("--headless")  # æ— å¤´æ¨¡å¼
# driver = webdriver.Edge(options=options)
#
# driver.get(PUBLICITY_URL)
#
# # âœ… è·å–å®Œæ•´ Cookies å¹¶ä¼ é€’ç»™ requests
# cookies = {cookie["name"]: cookie["value"] for cookie in driver.get_cookies()}
# response = requests.get(PUBLICITY_URL, headers=HEADERS, cookies=cookies)
# # å°è¯•è§£æ JSON
# try:
#     json_data = response.json()
#     print(json_data)
# except requests.exceptions.JSONDecodeError:
#     print("âŒ ä»ç„¶ä¸æ˜¯ JSONï¼Œè¿”å›çš„å†…å®¹:", response.text)

# driver.quit()
# driver.get(url=PUBLICITY_URL)
# # è·å– Selenium è‡ªåŠ¨ç”Ÿæˆçš„æ‰€æœ‰ Cookie
# cookies = driver.get_cookies()
# print(cookies)
# soup = BeautifulSoup(driver.page_source, "html.parser")
# print(driver.)

# # è·å–è®¸å¯ä¿¡æ¯å¹¶æå–è¯¦æƒ…ä¸å®æ—¶æ•°æ®
# permit_list = get_permits()[:20]
# for permit in permit_list:
#     # è·å–è¯¦æƒ…é¡µæ•°æ®
#     project_details = parse_project_details(permit["è¯¦æƒ…é¡µ"])
#     permit.update(project_details)
#
#     # è·å–å®æ—¶æ•°æ®
#     realdata = parse_realdata(permit["å®æ—¶æ•°æ®é¡µ"])
#     permit.update(realdata)
#     print(BASE_URL+permit["è®¸å¯è¯href"])
#     property_type = parse_property_type(BASE_URL+permit["è®¸å¯è¯href"])
#     permit.update(property_type)
#
# # å­˜å…¥ Excel
# df = pd.DataFrame(permit_list)
# df.to_excel("å®æ³¢æˆ¿äº§è®¸å¯-1.xlsx", index=False)
#
# print("æ•°æ®çˆ¬å–å®Œæˆï¼Œå·²ä¿å­˜åˆ° Excel æ–‡ä»¶ï¼")
