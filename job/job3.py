import random
import re
from datetime import datetime
from urllib.parse import urlparse, parse_qs

import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import pytesseract  # OCR è¯†åˆ«åº“
from io import BytesIO
from PIL import Image
from bs4 import BeautifulSoup
import cv2

pytesseract.pytesseract.tesseract_cmd = r"D:\2soft\ocr\tesseract.exe"
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.options import Options
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import numpy as np

# é…ç½®åŸºæœ¬ä¿¡æ¯
BASE_URL = "https://newhouse.cnnbfdc.com"
UNIT_DETAILS_URL = BASE_URL + "/unit/details?unitGUID="
PUBLICITY_URL = f"{BASE_URL}/publicity?page="
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36"
]
page=1
# OCR è¯†åˆ«ç›®æ ‡å…³é”®è¯
TARGET_KEYWORDS = ["ä½å®…", "åŠå…¬", "å‚æˆ¿", "å•†ä¸š", "å·¥ä¸š", "è½¦ä½", "å…¶ä»–"]
# HEADERS = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
#     "Referer": "https://newhouse.cnnbfdc.com/",
#     "Accept-Language": "zh-CN,zh;q=0.9",
#     "Accept-Encoding": "gzip, deflate, br",
# }
COOKIES = {
    "Hm_lvt_b33309ddse6aedc0b7a6a5e11379efd": "55048fdc8ccdde7cc81b097494237b631d7b88b3b7b320d38bca4987f33e87257ea3c7d9c0ed284586f83ee032b54d9ca7525d5c99e9af8d4258a7179c7dbff68a75d7eee825cdf6bd229954d0fbf68d98f3fd8c20440a70727d007056242e864c06b79de347d604207ba92c424c360f40cb50b8fde7a6a9524ab45050e895cffdd00fa57dddbbca4897a6a0ecfe803b2b9326b397106de82ceb4109fd11c06bd1afb1d62ccbec42914a51c85470207e",
}
COOKIES1 = [
    {"name": "Hm_lvt_b33309ddse6aedc0b7a6a5e11379efd",
     "value": "55048fdc8ccdde7cc81b097494237b631d7b88b3b7b320d38bca4987f33e87257ea3c7d9c0ed284586f83ee032b54d9ca7525d5c99e9af8d4258a7179c7dbff68a75d7eee825cdf6bd229954d0fbf68d98f3fd8c20440a70727d007056242e864c06b79de347d604207ba92c424c360f40cb50b8fde7a6a9524ab45050e895cffdd00fa57dddbbca4897a6a0ecfe803b2b9326b397106de82ceb4109fd11c06bd1afb1d62ccbec42914a51c85470207e"}
]

HEADERS = {"User-Agent": random.choice(USER_AGENTS),
           # "X-Requested-With": "XMLHttpRequest",
           }
EDGE_DRIVER_PATH = "D:/2soft/edgedriver/msedgedriver.exe"
# é…ç½® Selenium WebDriver

# é…ç½® Chrome
chrome_options = Options()
chrome_options.add_argument("--headless")  # æ— å¤´æ¨¡å¼
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-blink-features=AutomationControlled")  # å Selenium æ£€æµ‹
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

# service = Service("D:/2soft/chromedriver/chromedriver.exe")  # ä½ çš„ ChromeDriver è·¯å¾„
# driver = webdriver.Chrome(service=service, options=chrome_options)

options = Options()
options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
options.add_argument("--disable-gpu")

# æ‰‹åŠ¨æŒ‡å®š WebDriver ç«¯å£ï¼Œé¿å…ç«¯å£éšæœºå¯¼è‡´è¿æ¥å¤±è´¥
# service = EdgeService(EDGE_DRIVER_PATH)
# service.start()  # æ˜¾å¼å¯åŠ¨æœåŠ¡
# driver = webdriver.Edge(service=service, options=options)
# âœ… è®¾å®šå…¨å±€å˜é‡ï¼Œå­˜å‚¨ WebDriver
_driver = None


# def get_driver():
#     """è·å– WebDriver å®ä¾‹ï¼Œç¡®ä¿ WebDriver åªåˆå§‹åŒ–ä¸€æ¬¡"""
#     global _driver
#     if _driver is None:
#         options = Options()
#         options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
#         service = EdgeService("D:/2soft/edgedriver/msedgedriver.exe")  # EdgeDriver è·¯å¾„
#         _driver = webdriver.Edge(service=service, options=options)
#     return _driver
def get_driver():
    """æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„ WebDriverï¼Œç¡®ä¿ä¸ä¼šè¶…æ—¶"""
    options = Options()
    options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    service = EdgeService("D:/2soft/edgedriver/msedgedriver.exe")  # EdgeDriver è·¯å¾„
    driver = webdriver.Edge(service=service, options=options)
    return driver  # âš ï¸ å…³é”®å˜åŒ–ï¼šæ¯æ¬¡éƒ½è¿”å›ä¸€ä¸ªæ–°çš„ driver


def load_page_with_cookies(driver, url, cookies):
    """
    ä½¿ç”¨ Selenium åŠ è½½é¡µé¢ï¼Œå¹¶æ·»åŠ  Cookies ä»¥ä¿æŒä¼šè¯ã€‚

    :param driver: WebDriver å®ä¾‹
    :param url: ç›®æ ‡ç½‘é¡µ URL
    :param cookies: Cookies å­—å…¸
    """
    driver.get(url)  # æ‰“å¼€ç½‘é¡µ

    # âœ… é¿å…é‡å¤æ³¨å…¥ Cookies
    if not hasattr(driver, "_cookies_loaded"):
        cookies_list = [{"name": key, "value": value} for key, value in cookies.items()]
        for cookie in cookies_list:
            driver.add_cookie(cookie)
        driver.refresh()  # åˆ·æ–°é¡µé¢ï¼Œä½¿ Cookies ç”Ÿæ•ˆ
        driver._cookies_loaded = True  # æ ‡è®° Cookies å·²åŠ è½½

    # time.sleep(3)  # é€‚å½“ç­‰å¾…é¡µé¢åŠ è½½


# è·å–è®¸å¯è¯åˆ—è¡¨
def get_permits(limit=1000):
    response = requests.get(PUBLICITY_URL+str(page), headers=HEADERS, cookies=COOKIES)
    # time.sleep(1)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    permits = []
    table = soup.find("table", class_="layui-table")
    if not table:
        print("æœªæ‰¾åˆ°è®¸å¯ä¿¡æ¯è¡¨æ ¼")
        return []

    rows = table.find("tbody").find_all("tr")
    for row in rows[:limit]:  # é™åˆ¶è·å–çš„è®¸å¯è¯æ•°é‡
        columns = row.find_all("td")
        permit_name = columns[0].text.strip()
        permit_date = columns[1].text.strip()
        project_name = columns[2].text.strip()
        district = columns[3].text.strip()
        developer = columns[4].text.strip()
        # è·å–è¯¦æƒ…å’Œå®æ—¶æ•°æ®é¡µé¢é“¾æ¥
        detail_href = columns[0].find("a")["href"]
        detail_url = BASE_URL + detail_href
        realdata_url = detail_url.replace("Details", "realdata")
        # âœ… æå–å¹´ä»½å’Œæœˆä»½
        try:
            date_obj = datetime.strptime(permit_date, "%Y-%m-%d")
            year = date_obj.year
            month = date_obj.month
        except ValueError:
            year, month = "æœªçŸ¥", "æœªçŸ¥"
        permits.append({
            "è®¸å¯è¯åç§°": permit_name,
            "è®¸å¯æ—¥æœŸ": permit_date,
            "å¹´ä»½": year,  # âœ… æ–°å¢åˆ—
            "æœˆä»½": month,  # âœ… æ–°å¢åˆ—
            "é¡¹ç›®åç§°": project_name,
            "åŒºåŸŸ": district,
            "å¼€å‘ä¼ä¸š": developer,
            "è¯¦æƒ…é¡µ": detail_url,
            "å®æ—¶æ•°æ®é¡µ": realdata_url,
        })
    print(permits)
    return permits


def get_text_from_image(element):
    """æˆªå›¾å¹¶ä½¿ç”¨ OCR è§£æå¤‡æ¡ˆå¥—æ•°"""
    element.screenshot("å¤‡æ¡ˆå¥—æ•°.png")
    image = Image.open("å¤‡æ¡ˆå¥—æ•°.png")
    return pytesseract.image_to_string(image, config="--psm 6").strip()


def extract_number(text):
    """æå–å­—ç¬¦ä¸²ä¸­çš„æ•°å­—"""
    numbers = re.findall(r"\d+", text)
    return numbers[0] if numbers else "æœªçŸ¥"


# def get_text_from_image(img_url):
#     """ä¸‹è½½å›¾ç‰‡å¹¶ä½¿ç”¨ OCR æå–æ–‡å­—"""
#     try:
#         response = requests.get(img_url, headers=HEADERS, stream=True)
#         if response.status_code == 200:
#             image = Image.open(BytesIO(response.content))
#             text = pytesseract.image_to_string(image, config="--psm 6 digits")  # åªè¯†åˆ«æ•°å­—
#             return extract_number(text)
#         else:
#             return "å›¾ç‰‡ä¸‹è½½å¤±è´¥"
#     except Exception as e:
#         return f"OCR å¤±è´¥: {str(e)}"
# è§£æè¯¦æƒ…é¡µä¿¡æ¯
def parse_project_details(url):
    response = requests.get(url, headers=HEADERS, cookies=COOKIES)
    time.sleep(3)
    # if response.status_code != 200:
    #     return {"é¡¹ç›®åç§°": "è·å–å¤±è´¥"}
    print(response.status_code)
    soup = BeautifulSoup(response.text, "html.parser")
    details = {}

    # è·å–é¡¹ç›®åç§°
    # title_element = soup.find("big", class_="bold")
    # details["é¡¹ç›®åç§°"] = title_element.text.strip() if title_element else "æœªçŸ¥"

    # è·å–ä½å®…å¤‡æ¡ˆå‡ä»·
    price_element = soup.find("big", class_="color--red bold fs24")

    # æ‰¾åˆ°åŒ…å«"ä½å®…å¤‡æ¡ˆå‡ä»·"çš„ <li>
    price_li = soup.find("li", string=lambda text: text and "ä½å®…å¤‡æ¡ˆå‡ä»·" in text)

    # æå– <span class="price"> ä¸­çš„æ–‡æœ¬
    price = price_li.find("span", class_="price").text.strip() if price_li else "æœªçŸ¥"

    if re.search(r"\d+", price):
        details["ä½å®…å¤‡æ¡ˆå‡ä»·"] = price
        details["ä¸šæ€1"] = "ä½å®…"
    else:
        details["ä½å®…å¤‡æ¡ˆå‡ä»·"] = price_element.text.strip() if price_element else "æœªçŸ¥"
        details["ä¸šæ€1"] = ""
        # è·å–é¡¹ç›®åœ°å€
    address_element = soup.find("span", title=True)
    details["é¡¹ç›®åœ°å€"] = address_element.text.strip() if address_element else "æœªçŸ¥"
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… "è¡—é“"ï¼ˆå¿½ç•¥ç©ºæ ¼ã€HTMLå®ä½“ï¼‰
    street_element = soup.find("em", string=re.compile(r"è¡—\s*é“"))

    # è·å–è¡—é“ä¿¡æ¯
    details["æ‰€åœ¨åŒº"] = street_element.find_next_sibling("span").text.strip() if street_element else "æœªçŸ¥"
    # è·å–å¼€å‘å•†
    # developer_element = soup.find("span").find("a")
    # details["å¼€å‘ä¼ä¸š"] = developer_element.text.strip() if developer_element else "æœªçŸ¥"
    # æå–å»ºç­‘ç±»å‹
    # è·å–å»ºç­‘ç±»å‹
    building_type = soup.find("em", string="å»ºç­‘ç±»å‹ï¼š")
    details["å»ºç­‘ç±»å‹"] = building_type.find_next_sibling("span").text.strip() if building_type else "æœªçŸ¥"

    # è·å–è£…ä¿®æ ‡å‡†
    decoration_standard = soup.find("em", string="è£…ä¿®æ ‡å‡†ï¼š")
    details["è£…ä¿®æ ‡å‡†"] = decoration_standard.find_next_sibling("span").text.strip() if decoration_standard else "æœªçŸ¥"
    # è·å–å»ºç­‘é¢ç§¯
    area_element = soup.find("li", string=lambda text: text and "å»ºç­‘é¢ç§¯ï¼š" in text)
    details["é¢„å”®å»ºç­‘é¢ç§¯"] = area_element.text.split("ï¼š")[-1].strip() if area_element else "æœªçŸ¥"
    # âœ… è·å–é¡¹ç›®ç®€ä»‹
    profile_input = soup.find("input", id="proj-text")
    details["é¡¹ç›®ç®€ä»‹"] = profile_input["value"].strip() if profile_input else "æœªçŸ¥"
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å« "è®¸å¯è¯å·" çš„ <a> æ ‡ç­¾
    first_permit = soup.find("a", href=True, string=lambda text: text and permit["è®¸å¯è¯åç§°"] in text)
    details["è®¸å¯è¯href"] = first_permit["href"] if first_permit else "æœªçŸ¥"
    # æŸ¥æ‰¾åŒ…å«ç»çº¬åº¦çš„ <a> æ ‡ç­¾
    a_tag = soup.find("div", id="map-location").find("a")

    # æå– href ä¸­çš„å‚æ•°
    href = a_tag['href']
    query = urlparse(href).query
    params = parse_qs(query)

    # æå–ç»çº¬åº¦
    lat = params.get("Lat", [None])[0]
    lng = params.get("Lng", [None])[0]
    details["ç»åº¦"] =lng if lng else "æœªçŸ¥"
    details["çº¬åº¦"] =lat if lat else "æœªçŸ¥"


    try:
        # âœ… è·å– WebDriverï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        driver = get_driver()
        load_page_with_cookies(driver, url, COOKIES)
        # å…ˆç”¨ requests è·å– Cookie
        # session = requests.Session()
        # session.get(url, headers={"User-Agent": "Mozilla/5.0"})
        # cookies = session.cookies.get_dict()
        # print(cookies)
        # ä¼ é€’ Cookie ç»™ Selenium
        # **éå† COOKIES_LIST é€ä¸ªæ·»åŠ **
        # driver.get(url)
        # COOKIES_LIST = [{"name": key, "value": value} for key, value in COOKIES.items()]
        # # print(COOKIES_LIST)
        # for cookie in COOKIES_LIST:
        #     driver.add_cookie(cookie)  # âœ… ç¡®ä¿æ ¼å¼æ­£ç¡®
        #
        # time.sleep(3)  # é€‚å½“ç­‰å¾…é¡µé¢åŠ è½½
        # è·å–å¤‡æ¡ˆå¥—æ•°
        try:
            # img_element = driver.find_element(By.CLASS_NAME, "totalresidentialhousingcount")
            # details["é¢„å”®å¥—æ•°"] = get_text_from_image(img_element)
            # âœ… ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

            # âœ… ç­‰å¾…ç›®æ ‡å›¾ç‰‡å…ƒç´ å¯è§
            img_element = WebDriverWait(driver, 10).until(
                EC.visibility_of_element_located((By.CLASS_NAME, "totalresidentialhousingcount"))
            )

            # âœ… é‡æ–°æ£€æŸ¥å°ºå¯¸
            # width = img_element.size["width"]
            # height = img_element.size["height"]
            # if width == 0 or height == 0:
            #     return "OCR è§£æå¤±è´¥: å›¾ç‰‡å°ºå¯¸ä¸º 0"

            # âœ… æˆªå›¾å¹¶ OCR è§£æ
            details["é¢„å”®å¥—æ•°"] = get_text_from_image(img_element)
        except Exception as e:
            print(f"OCR è§£æå¤±è´¥: {e}")
            details["é¢„å”®å¥—æ•°"] = "OCR è§£æå¤±è´¥"

        return details
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return details


# è§£æå®æ—¶æ•°æ®é¡µä¿¡æ¯
@retry(
    stop=stop_after_attempt(5),  # æœ€å¤§é‡è¯• 5 æ¬¡
    wait=wait_exponential(multiplier=1, min=2, max=30),  # æŒ‡æ•°é€€é¿ï¼ˆ2s, 4s, 8s...æœ€å¤š 30sï¼‰
    retry=retry_if_exception_type(Exception)  # åªå¯¹ Exception è¿›è¡Œé‡è¯•
)
def parse_realdata(url):
    # print(1)
    response = requests.get(url, headers=HEADERS, cookies=COOKIES)
    time.sleep(4)
    if response.status_code == 503:
        print("503 æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè§¦å‘é‡è¯•...")
        raise Exception("503 Service Unavailable")  # è§¦å‘é‡è¯•
    # if response.status_code != 200:
    #     return {"è®¸å¯é¢ç§¯": "è·å–å¤±è´¥", "å·²å”®é¢ç§¯": "è·å–å¤±è´¥", "å½“å‰å¯å”®å¥—æ•°": "è·å–å¤±è´¥"}
    print(response.status_code)
    soup = BeautifulSoup(response.text, "html.parser")
    realdata = {}

    # è·å–è®¸å¯ã€å·²å”®ã€å¯å”®æ•°æ®
    data_summary = soup.find("ul", class_="data-summary-list")
    if data_summary:
        items = data_summary.find_all("li")
        # realdata["é¢„å”®å»ºç­‘é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰"] = extract_number(items[1].text.strip() if len(items) > 1 else "æœªçŸ¥")
        # realdata["é¢„å”®å¥—æ•°"] = extract_number(items[-2].text.strip() if len(items) > 2 else "æœªçŸ¥")
        realdata["è®¸å¯è¯é¢ç§¯"] = extract_number(items[0].text.strip() if len(items) > 0 else "æœªçŸ¥")
        realdata["è®¸å¯å¥—æ•°"] = extract_number(items[6].text.strip() if len(items) > 6 else "æœªçŸ¥")
    return realdata


@retry(
    stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯• 3 æ¬¡
    wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿ï¼ˆ2s, 4s, 8sï¼‰
    retry=retry_if_exception_type(requests.exceptions.RequestException),  # åªå¯¹è¯·æ±‚å¼‚å¸¸é‡è¯•
    retry_error_callback=lambda _: {"ä¸šæ€": "æœªçŸ¥"}  # å½“ 3 æ¬¡éƒ½å¤±è´¥æ—¶è¿”å›é»˜è®¤å€¼
)
def parse_property_type(url):
    """
    è§£ææ¥¼ç›˜è¯¦æƒ…é¡µï¼Œè·å–ä»»æ„æˆ¿å·çš„ `data-guid`ï¼Œè®¿é—® `unit/details` é¡µé¢ï¼Œ
    OCR è¯†åˆ«æˆ¿å±‹ä¸šæ€ï¼ˆå¦‚ ä½å®…ã€åŠå…¬ç­‰ï¼‰ã€‚
    :param url: æ¥¼ç›˜ä¿¡æ¯é¡µé¢ URL
    :return: ä¸šæ€ä¿¡æ¯ï¼Œå¦‚ `{"ä¸šæ€": "ä½å®…"}`
    """
    details = {"ä¸šæ€": permit["ä¸šæ€1"]}  # é»˜è®¤å€¼
    headers = HEADERS.copy()  # é¿å…å…¨å±€ä¿®æ”¹
    headers["Referer"] = url
    # 1. è®¿é—®æ¥¼ç›˜ä¿¡æ¯é¡µ
    response = requests.get(url, headers=HEADERS, cookies=COOKIES)
    time.sleep(4)
    if response.status_code == 503:
        print("503 æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè§¦å‘é‡è¯•...")
        raise Exception("503 Service Unavailable")  # è§¦å‘é‡è¯•
    result = {"ä¸šæ€": "æœªçŸ¥"}  # è®¾å®šé»˜è®¤å€¼ï¼Œç»§ç»­æ‰§è¡Œç¨‹åº
    # 2. æå– `data-guid`
    soup = BeautifulSoup(response.text, "html.parser")
    # print(soup.prettify())
    house_links = soup.select(".house-title a[data-guid]")  # æ‰€æœ‰æˆ¿å±‹é“¾æ¥
    if not house_links:
        print("âš ï¸ æœªæ‰¾åˆ°æˆ¿å· `data-guid`ï¼Œè·³è¿‡è§£æ")
        return details
    unit_guid = house_links[0].get("data-guid")

    # 3. è®¿é—® `unit/details`
    unit_url = UNIT_DETAILS_URL + unit_guid
    print(f"ğŸ” è·å–å•å…ƒè¯¦æƒ…é¡µ: {unit_url}")

    try:
        unit_response = requests.get(unit_url, headers=headers, cookies=COOKIES)
        unit_response.raise_for_status()
        unit_soup = BeautifulSoup(unit_response.text, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"âŒ å•å…ƒè¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {e}")
        return details

    # 4. æå–å›¾ç‰‡ URL
    img_element = unit_soup.find("img")
    if not img_element or "src" not in img_element.attrs:
        print("âš ï¸ æœªæ‰¾åˆ°æˆ¿å±‹å›¾ç‰‡ï¼Œè·³è¿‡ OCR è¯†åˆ«")
        return details
    img_url = img_element["src"]

    # å¤„ç†ç›¸å¯¹è·¯å¾„
    if img_url.startswith("/"):
        img_url = BASE_URL + img_url

    # 5. OCR è¯†åˆ«
    try:
        img_response = requests.get(img_url, headers=HEADERS, stream=True, cookies=COOKIES)
        if img_response.status_code == 200:
            # è¯»å–å›¾ç‰‡æ•°æ®å¹¶è½¬æ¢ä¸º OpenCV æ ¼å¼
            image = Image.open(BytesIO(img_response.content))
            image_np = np.array(image)  # PIL â†’ NumPy
            # gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)  # è½¬æ¢ä¸ºç°åº¦å›¾

            # é¢„å¤„ç†ï¼šå»å™ª & å¢å¼ºå¯¹æ¯”åº¦
            # blurred = cv2.GaussianBlur(gray, (3, 3), 0)
            # binary = cv2.adaptiveThreshold(
            #     blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
            # )

            # OCR è¯†åˆ«
            text = pytesseract.image_to_string(image, lang="chi_sim", config="--psm 6").strip()
            # print(image)
            # print(text)
        else:
            print("âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥")
            return details
    except Exception as e:
        print(f"âš ï¸ OCR å¤±è´¥: {e}")
        return details

    # 6. ç¡®å®šä¸šæ€
    for keyword in TARGET_KEYWORDS:
        if keyword in text:
            details["ä¸šæ€"] = keyword
            break  # æ‰¾åˆ°å³åœæ­¢
    return details


# è·å–è®¸å¯ä¿¡æ¯å¹¶æå–è¯¦æƒ…ä¸å®æ—¶æ•°æ®
all_data = []
pages = 3
for page1 in range(1, pages + 1):
    page = page1
    permit_list = get_permits()[:20]
    for permit in permit_list:
        # è·å–è¯¦æƒ…é¡µæ•°æ®
        project_details = parse_project_details(permit["è¯¦æƒ…é¡µ"])
        permit.update(project_details)

        # è·å–å®æ—¶æ•°æ®
        realdata = parse_realdata(permit["å®æ—¶æ•°æ®é¡µ"])
        permit.update(realdata)
        print(BASE_URL + permit["è®¸å¯è¯href"])
        property_type = parse_property_type(BASE_URL + permit["è®¸å¯è¯href"])
        permit.update(property_type)

    # å­˜å…¥ Excel
    all_data.extend(permit_list)

df = pd.DataFrame(all_data)
df.to_excel("å®æ³¢æˆ¿äº§è®¸å¯.xlsx", index=False)

print("æ•°æ®çˆ¬å–å®Œæˆï¼Œå·²ä¿å­˜åˆ° Excel æ–‡ä»¶ï¼")
